{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The code here is refernced from https://github.com/awjuliani/DeepRL-Agents/blob/master/Contextual-Policy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining contextual bandits, four four arm bandits\n",
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        #List out our bandits. Currently arms 4, 2, 1 and 1 (respectively) are the most optimal.\n",
    "        self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5],[-5,0.2,0,1]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0,len(self.bandits)) #Returns a random state for each episode.\n",
    "        return self.state\n",
    "        \n",
    "    def pullArm(self,action):\n",
    "        #Get a random number.\n",
    "        bandit = self.bandits[self.state,action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            #return a positive reward.\n",
    "            return 1\n",
    "        else:\n",
    "            #return a negative reward.\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining Policy based agent. It takes input current state and retruns action\n",
    "class agent():\n",
    "    def __init__(self, lr, s_size,a_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in= tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in,s_size)\n",
    "        output = slim.fully_connected(state_in_OH,a_size,\\\n",
    "            biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())\n",
    "        self.output = tf.reshape(output,[-1])\n",
    "        self.chosen_action = tf.argmax(self.output,0)\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output,self.action_holder,[1])\n",
    "        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 4 bandits: [ 0.    0.25  0.    0.  ]\n",
      "Mean reward for each of the 4 bandits: [  3.5   19.5   30.25  31.  ]\n",
      "Mean reward for each of the 4 bandits: [  5.5   48.5   59.    62.75]\n",
      "Mean reward for each of the 4 bandits: [  8.75  75.25  83.75  91.5 ]\n",
      "Mean reward for each of the 4 bandits: [   7.5   100.5   109.5   120.25]\n",
      "Mean reward for each of the 4 bandits: [  16.5   130.5   133.25  148.5 ]\n",
      "Mean reward for each of the 4 bandits: [  47.5   157.25  157.75  176.25]\n",
      "Mean reward for each of the 4 bandits: [  76.    184.25  186.    207.5 ]\n",
      "Mean reward for each of the 4 bandits: [ 104.75  214.25  212.25  236.5 ]\n",
      "Mean reward for each of the 4 bandits: [ 133.    240.25  242.25  263.25]\n",
      "Mean reward for each of the 4 bandits: [ 156.    271.25  269.75  292.75]\n",
      "Mean reward for each of the 4 bandits: [ 186.75  301.75  298.75  317.5 ]\n",
      "Mean reward for each of the 4 bandits: [ 212.25  329.25  328.    347.25]\n",
      "Mean reward for each of the 4 bandits: [ 240.25  360.    353.    376.  ]\n",
      "Mean reward for each of the 4 bandits: [ 270.25  391.5   373.25  403.75]\n",
      "Mean reward for each of the 4 bandits: [ 293.25  421.25  401.25  433.5 ]\n",
      "Mean reward for each of the 4 bandits: [ 322.25  448.5   428.75  460.75]\n",
      "Mean reward for each of the 4 bandits: [ 350.25  477.75  454.25  489.  ]\n",
      "Mean reward for each of the 4 bandits: [ 376.    511.25  478.    518.  ]\n",
      "Mean reward for each of the 4 bandits: [ 404.75  537.75  505.25  546.5 ]\n",
      "The agent thinks action 4 for bandit 1 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 2 for bandit 2 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 3 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks action 1 for bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "#Training network by getting state from environment, take an action and receive reward\n",
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "\n",
    "cBandit = contextual_bandit() #Load the bandits.\n",
    "myAgent = agent(lr=0.001,s_size=cBandit.num_bandits,a_size=cBandit.num_actions) #Load the agent.\n",
    "weights = tf.trainable_variables()[0] #The weights we will evaluate to look into the network.\n",
    "\n",
    "total_episodes = 10000 #Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros([cBandit.num_bandits,cBandit.num_actions]) #Set scoreboard for bandits to 0.\n",
    "e = 0.1 #Set the chance of taking a random action.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = cBandit.getBandit() #Get a state from the environment.\n",
    "        \n",
    "        #Choose either a random action or one from our network.\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action,feed_dict={myAgent.state_in:[s]})\n",
    "        \n",
    "        reward = cBandit.pullArm(action) #Get our reward for taking an action given a bandit.\n",
    "        \n",
    "        #Update the network.\n",
    "        feed_dict={myAgent.reward_holder:[reward],myAgent.action_holder:[action],myAgent.state_in:[s]}\n",
    "        _,ww = sess.run([myAgent.update,weights], feed_dict=feed_dict)\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        total_reward[s,action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print(\"Mean reward for each of the \" + str(cBandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1)))\n",
    "        i+=1\n",
    "for a in range(cBandit.num_bandits):\n",
    "    print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n",
    "    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
    "        print(\"...and it was right!\")\n",
    "    else:\n",
    "        print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now assuming each bandit to be person and each arm to be a genre of film. \n",
    "#Creating a dataset which includes ratings of 4 persons for 4 different movie genres\n",
    "import pandas as pd\n",
    "genre_names = [\"Adventure\", \"Sci-Fi\", \"Romance\", \"Horror\"]\n",
    "person_names = [\"person1\", \"person2\", \"person3\", \"person4\"]\n",
    "df = pd.DataFrame(index = person_names, columns = genre_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let person 1 rate his preferences of genre on a scale of 5\n",
    "df.loc['person1'] = pd.Series({'Adventure':1, 'Sci-Fi':5, 'Romance':2, 'Horror':1})\n",
    "#Similarly for others\n",
    "df.loc['person2'] = pd.Series({'Adventure':1, 'Sci-Fi':2, 'Romance':1, 'Horror':5})\n",
    "df.loc['person3'] = pd.Series({'Adventure':5, 'Sci-Fi':3, 'Romance':2, 'Horror':2})\n",
    "df.loc['person4'] = pd.Series({'Adventure':1, 'Sci-Fi':2, 'Romance':5, 'Horror':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Horror</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Adventure  Sci-Fi  Romance  Horror\n",
       "person1          1       5        2       1\n",
       "person2          1       2        1       5\n",
       "person3          5       3        2       2\n",
       "person4          1       2        5       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now repersenting this dataset in terms of bandit problem. Lower the bandit number more likely a positive reward.\n",
    "#Convert all positive scores to negative and train the bandit problem as defined above.\n",
    "class contextual_bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        #Assuming bandits to be persons and arms as genre. Currently genres Sci-Fi,Horror,Adventure,Romance are \n",
    "        #the most optimal respectively\n",
    "        self.bandits = np.array([[-1,-5,-2,-1],[-1,-2,-1,-5],[-5,-3,-2,-2],[-1,-2,-5,-1]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0,len(self.bandits)) #Returns a random state for each episode.\n",
    "        return self.state\n",
    "        \n",
    "    def pullArm(self,action):\n",
    "        #Get a random number.\n",
    "        bandit = self.bandits[self.state,action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            #return a positive reward.\n",
    "            return 1\n",
    "        else:\n",
    "            #return a negative reward.\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for each of the 4 persons: [ 0.    0.25  0.    0.  ]\n",
      "Mean reward for each of the 4 persons: [ 16.25  23.25  32.75  18.5 ]\n",
      "Mean reward for each of the 4 persons: [ 36.5   47.    64.25  39.5 ]\n",
      "Mean reward for each of the 4 persons: [ 57.    70.75  94.5   60.5 ]\n",
      "Mean reward for each of the 4 persons: [  79.     88.5   127.75   78.5 ]\n",
      "Mean reward for each of the 4 persons: [ 102.75  109.5   162.5    96.  ]\n",
      "Mean reward for each of the 4 persons: [ 131.25  133.    187.75  117.75]\n",
      "Mean reward for each of the 4 persons: [ 157.    152.5   216.5   142.25]\n",
      "Mean reward for each of the 4 persons: [ 178.5   179.5   247.    162.75]\n",
      "Mean reward for each of the 4 persons: [ 202.5   204.25  281.    183.5 ]\n",
      "Mean reward for each of the 4 persons: [ 224.25  231.    311.25  200.25]\n",
      "Mean reward for each of the 4 persons: [ 250.5   252.25  344.    218.5 ]\n",
      "Mean reward for each of the 4 persons: [ 272.75  273.    375.    237.5 ]\n",
      "Mean reward for each of the 4 persons: [ 292.25  292.25  408.    258.25]\n",
      "Mean reward for each of the 4 persons: [ 310.25  312.25  440.25  280.5 ]\n",
      "Mean reward for each of the 4 persons: [ 330.75  335.    474.    303.  ]\n",
      "Mean reward for each of the 4 persons: [ 351.75  355.5   502.    327.5 ]\n",
      "Mean reward for each of the 4 persons: [ 372.75  373.25  532.5   347.25]\n",
      "Mean reward for each of the 4 persons: [ 398.5   392.75  567.75  368.75]\n",
      "Mean reward for each of the 4 persons: [ 422.    414.5   598.5   390.25]\n",
      "The agent thinks genre 1 for person 1 is the most promising....\n",
      "...and it was wrong!\n",
      "The agent thinks genre 3 for person 2 is the most promising....\n",
      "...and it was wrong!\n",
      "The agent thinks genre 1 for person 3 is the most promising....\n",
      "...and it was right!\n",
      "The agent thinks genre 1 for person 4 is the most promising....\n",
      "...and it was wrong!\n"
     ]
    }
   ],
   "source": [
    "#Training network by getting state from environment, take an action and receive reward\n",
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "\n",
    "cBandit = contextual_bandit() #Load the bandits.\n",
    "myAgent = agent(lr=0.001,s_size=cBandit.num_bandits,a_size=cBandit.num_actions) #Load the agent.\n",
    "weights = tf.trainable_variables()[0] #The weights we will evaluate to look into the network.\n",
    "\n",
    "total_episodes = 10000 #Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros([cBandit.num_bandits,cBandit.num_actions]) #Set scoreboard for bandits to 0.\n",
    "e = 0.1 #Set the chance of taking a random action.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = cBandit.getBandit() #Get a state from the environment.\n",
    "        \n",
    "        #Choose either a random action or one from our network.\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(cBandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(myAgent.chosen_action,feed_dict={myAgent.state_in:[s]})\n",
    "        \n",
    "        reward = cBandit.pullArm(action) #Get our reward for taking an action given a bandit.\n",
    "        \n",
    "        #Update the network.\n",
    "        feed_dict={myAgent.reward_holder:[reward],myAgent.action_holder:[action],myAgent.state_in:[s]}\n",
    "        _,ww = sess.run([myAgent.update,weights], feed_dict=feed_dict)\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        total_reward[s,action] += reward\n",
    "        if i % 500 == 0:\n",
    "            print(\"Mean reward for each of the \" + str(cBandit.num_bandits) + \" persons: \" + str(np.mean(total_reward,axis=1)))\n",
    "        i+=1\n",
    "for a in range(cBandit.num_bandits):\n",
    "    print(\"The agent thinks genre \" + str(np.argmax(ww[a])+1) + \" for person \" + str(a+1) + \" is the most promising....\")\n",
    "    if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]):\n",
    "        print(\"...and it was right!\")\n",
    "    else:\n",
    "        print(\"...and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It does not get it right for all persons.I am assuming the use case might be wrong or data needs some pre-processing.\n",
    "#Will have to learn more about it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
