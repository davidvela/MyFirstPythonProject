{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP\n",
    "Start writing all my logic in jupyter notebook in order to run them from anaconda! <br>\n",
    "** IMPORTANT ** <br>\n",
    "the network cell should only be intialize once! otherwise the program start creating indexes for the variables!!!! \n",
    "\n",
    "## index: \n",
    "<a id='index'/>\n",
    "\n",
    "1. READ DATA \n",
    "    * Class \n",
    "    * files \n",
    "2. [Model](#model)\n",
    "3. [Train](#tr) \n",
    "4. [Evaluate](#ev) \n",
    "5. [Test](#ts) \n",
    "6. [Other](#o)\n",
    "\n",
    "[go back](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from types import *\n",
    "from collections import Counter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mData as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___Start!___13:22:15\n",
      "FRFLO_C4_filt:0\n",
      "data read - lenTrain=4610-4610 & lenEv=5000-5000 time:5.188518762588501\n",
      "N of columns: 1814\n",
      "___Data Read!\n"
     ]
    }
   ],
   "source": [
    "print(\"___Start!___\" +  datetime.now().strftime('%H:%M:%S')  )\n",
    "# md.spn = 200\n",
    "ninp, nout  = md.mainRead()\n",
    "# For test I am forced to used JSON - column names and order may be different! \n",
    "#  md.DESC     = \"FREXP\"\n",
    "# ninp, nout  = md.mainRead2()\n",
    "print(\"___Data Read!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mData.dataE[\"label\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# md.MODEL_DIR  = sss\n",
    "model_path    = md.MODEL_DIR \n",
    "lr         = 0.01\n",
    "h          = [40 , 10]\n",
    "epochs     = 30\n",
    "disp       = 5\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nns(): return str(ninp)+'*'+str(h[0])+'*'+str(h[1])+'*'+str(nout)\n",
    "def logr(datep = '' , time='', it=1000, nn='', typ='TR', DS='', AC=0, num=0, AC3=0, AC10=0, desc='', timeStart=''):\n",
    "    if desc == '': print(\"Log not recorded\"); return \n",
    "    LOG = \"../../_zfp/LOGT2.txt\"\n",
    "    f= open(LOG ,\"a+\") #w,a,\n",
    "    if datep != '':   dats = datep\n",
    "    else:             dats = datetime.now().strftime('%d.%m.%Y') \n",
    "    if time != '':    times = time\n",
    "    else:             times = datetime.now().strftime('%H:%M:%S') \n",
    "\n",
    "    line =  datetime.now().strftime('%d.%m.%Y') + '\\t' + times \n",
    "    line = line + '\\t' + str(it) + '\\t'+  get_nns() +  '\\t' + str(lr)\n",
    "    line = line + '\\t' + typ \n",
    "    line = line + '\\t' + str(DS) + '\\t' + str(AC) + '\\t' + str(num) + '\\t' + str(AC3) + '\\t' +  str(AC10) + '\\t' + desc \n",
    "    line = line + '\\t' + str(batch_size) + '\\t' +  timeStart + '\\n' #new\n",
    "\n",
    "    f.write(line);  f.close()\n",
    "    print(\"___Log recorded\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1814*40*10*4\n",
      "___Network created\n"
     ]
    }
   ],
   "source": [
    "# NETWORK-----------------------------------------------------\n",
    "print( get_nns() )\n",
    "x = tf.placeholder(tf.float32,   shape=[None, ninp],              name=\"x\")\n",
    "y = tf.placeholder(tf.int16,     shape=[None, nout],              name=\"y\")\n",
    "biases  = { 'b1': tf.Variable(tf.random_normal( [ h[0] ]),        name=\"Bias_1\"),\n",
    "                'b2': tf.Variable(tf.random_normal( [ h[1] ]),    name=\"Bias_2\"),\n",
    "                'out': tf.Variable(tf.random_normal( [nout] ),    name=\"Bias_out\") }\n",
    "weights = { 'h1': tf.Variable(tf.random_normal([ninp,h[0]]),      name=\"Weights_1\"),\n",
    "            'h2': tf.Variable(tf.random_normal([h[0],h[1]]),      name=\"Weights_2\"),\n",
    "            'out': tf.Variable(tf.random_normal([h[1], nout]),    name=\"Weights_out\")}\n",
    "\n",
    "def build_network1( ):\n",
    "    with tf.name_scope(\"fc_1\"):\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    with tf.name_scope(\"fc_2\"):\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    with tf.name_scope(\"fc_output\"):\n",
    "        pred = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    softmaxT = tf.nn.softmax(pred, )\n",
    "    prediction=tf.reduce_max(y,1)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "        tf.summary.scalar(\"xent\", cost)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "    \n",
    "    summ = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver= tf.train.Saver()\n",
    "    return init, pred, accuracy, cost, optimizer, saver, softmaxT\n",
    "init, prediction, accuracy, cost, optimizer, saver, softmaxT = build_network1()\n",
    "\n",
    "def restore_model(sess):    \n",
    "    print(\"Model restored from file: %s\" % model_path)\n",
    "    saver.restore(sess, model_path)\n",
    "print(\"___Network created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____TESTS...\n",
      "input-no=1814\n",
      "cycle: 0\n",
      "Counter of comp. not included :\n",
      "Counter()\n",
      "[[0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    " url_test = \"../../_zfp/data/FREXP/\" ; md.DESC     = \"FREXP\"\n",
    "\n",
    "\n",
    "print(\"_____TESTS...\")    \n",
    "dataTest = {'label' : [] , 'data' :  [] };     pred_val = []\n",
    "\n",
    "if url_test != 'url':  \n",
    "    json_data = url_test + \"data_json1.txt\"\n",
    "    tmpLab = pd.read_csv(url_test + \"datal1.csv\", sep=',', usecols=[0,1])    \n",
    "    tmpLab = tmpLab.loc[:,'fp']\n",
    "else: \n",
    "    json_str = '''[{ \"m\":\"8989\", \"c1\" :0.5 },\n",
    "        { \"m\":\"8988\", \"c3\" :0.5 , \"c4\" :0.5 }] '''\n",
    "    json_data = json.loads(json_str)\n",
    "    tmpLab = [59,99]\n",
    "dataTest['data']  = md.feed_data(json_data, True , d_st=False)\n",
    "[dataTest['label'].append( md.cc(x) ) for x in tmpLab ]\n",
    "# print(dataTest['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
